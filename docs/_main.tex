% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Data Analysis in R: A Basic Guide},
  pdfauthor={Antonio Solorio},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Data Analysis in R: A Basic Guide}
\author{Antonio Solorio}
\date{2024-07-19}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Welcome to ``Data Analysis in R: A Basic Guide.'' This book aims to provide you with a basic foundation in data analysis using R, a powerful and versatile programming language. Throughout this book, you will learn various techniques and tools essential for effective data analysis.

To illustrate these concepts, we will use the Home Mortgage Disclosure Act (HMDA) data as a practical example. This real-world dataset will help you understand how to apply data analysis methods in a meaningful context.

\hypertarget{what-is-hmda-data}{%
\chapter{What is HMDA Data?}\label{what-is-hmda-data}}

The Home Mortgage Disclosure Act (HMDA) was enacted by Congress in 1975 and is implemented by the Consumer Financial Protection Bureau (CFPB). The HMDA requires many financial institutions to maintain, report, and publicly disclose information about mortgages. This information is crucial for understanding and monitoring trends in housing finance, and for ensuring compliance with fair lending laws. \footnote{If you would like to learn more about HMDA data please see: \url{https://www.consumerfinance.gov/data-research/hmda/}}

HMDA data includes information on loan applications, loan originations, loan purchases, and denied applications. The data encompasses various aspects such as:

\begin{itemize}
\tightlist
\item
  \textbf{Loan Characteristics}: Information about the loan amount, type of loan, and purpose of the loan (e.g., home purchase, refinance).
\item
  \textbf{Applicant Information}: Demographic details of the loan applicants including race, ethnicity, gender, and income.
\item
  \textbf{Property Information}: Data about the location and type of property being financed.
\item
  \textbf{Action Taken}: The outcome of the loan application, whether it was approved, denied, or withdrawn.
\end{itemize}

\hypertarget{why-use-hmda-data}{%
\section{Why Use HMDA Data?}\label{why-use-hmda-data}}

While this book is focused on teaching data analysis in R, the HMDA dataset serves as an excellent example for several reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Real-World Relevance}: HMDA data provides a real-world context that makes the learning process more engaging and practical.
\item
  \textbf{Comprehensive Dataset}: The dataset includes a wide range of variables, making it suitable for demonstrating various data analysis techniques.
\item
  \textbf{Publicly Available}: HMDA data is publicly accessible, allowing you to follow along with the examples and practice on your own.
\end{enumerate}

By the end of this book, you will not only have a solid understanding of data analysis in R but also be equipped with practical skills that can be applied to other datasets and domains.

Let's get started on this journey of exploring data analysis with R, using the HMDA data as our guide!

\hypertarget{data-importing}{%
\chapter{Data Importing}\label{data-importing}}

In this chapter, we will explore the process of importing data into R for analysis. Data import is a crucial step in the data analysis workflow, as it allows you to load external data into R for further processing and analysis. We will focus on importing data from CSV files, which are one of the most common data formats used in data analysis. We will also discuss common issues encountered during data import and how to handle them, and how to handle the importation of large datasets in chunks.

\hypertarget{different-types-of-data}{%
\section{Different Types of Data}\label{different-types-of-data}}

In the realm of data analysis, you will encounter various types of data formats. Here are some common ones:

\begin{itemize}
\tightlist
\item
  \textbf{Text Files}: Unstructured text data that can be read line by line or in blocks, and which may be delimited by specific characters.
\item
  \textbf{CSV (Comma-Separated Values)}: A CSV file is a type of text file that is delimited by commas. It is one of the most common data formats used for storing tabular data.
\item
  \textbf{Excel Files}: Commonly used spreadsheets saved in formats like \texttt{.xlsx} or \texttt{.xls}.
\item
  \textbf{JSON (JavaScript Object Notation)}: A lightweight data interchange format that is easy for humans to read and write and easy for machines to parse and generate.
\item
  \textbf{SQL Databases}: Structured data stored in relational databases, which can be queried using SQL (Structured Query Language).
\item
  \textbf{API Data}: Data fetched from web APIs, which often come in formats like JSON or XML.
\end{itemize}

\hypertarget{why-start-with-csv-files}{%
\subsection{Why Start with CSV Files?}\label{why-start-with-csv-files}}

We will start with CSV files for several reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Simplicity}: CSV files are easy to understand and work with, making them ideal for beginners.
\item
  \textbf{Ubiquity}: CSV is one of the most common data formats, widely supported by various applications and programming languages.
\item
  \textbf{Ease of Use in R}: R provides straightforward functions for importing and handling CSV files, making it an excellent starting point for learning data import techniques.
\end{enumerate}

By mastering the import of CSV files, you'll build a strong foundation that will make it easier to work with other data formats as you progress in your data analysis journey.

\hypertarget{downloading-and-importing-hmda-data-in-csv-format}{%
\section{Downloading and Importing HMDA Data in CSV Format}\label{downloading-and-importing-hmda-data-in-csv-format}}

To practice importing CSV files in R, we will use the Home Mortgage Disclosure Act (HMDA) data in CSV format. This data can be found at the \href{https://ffiec.cfpb.gov/}{Consumer Financial Protection Bureau (CFPB) website}. In particular we will be working with the Snapshot National Loan Level Dataset, specifically for that in 2022 for Nevada.

\hypertarget{snapshot-national-loan-level-dataset}{%
\subsection{Snapshot National Loan Level Dataset}\label{snapshot-national-loan-level-dataset}}

The Snapshot files contain the national HMDA datasets as of May 1, 2023 for all HMDA reporters, as modified by the Bureau to protect applicant and borrower privacy. The snapshot files are available to download in both .csv and pipe delimited text file formats at the following link: \url{https://ffiec.cfpb.gov/data-publication/snapshot-national-loan-level-dataset/}. One of the issues with these files however is that they are quite large, so we will be working with a subset of the data for Nevada in 2022.

The subset of the data for Nevada in 2022 can be downloaded from the following link: \href{https://ffiec.cfpb.gov/v2/data-browser-api/view/csv?states=NV\&years=2022}{Nevada 2022 HMDA Data}.

\hypertarget{importing-csv-files-in-r}{%
\section{Importing CSV Files in R}\label{importing-csv-files-in-r}}

R provides several functions for importing CSV files. The most commonly used function is \textbf{\texttt{read.csv()}}, which is part of the base R package. Additionally, the \textbf{\texttt{readr}} package offers the \textbf{\texttt{read\_csv()}} function, which is optimized for faster performance and easier handling of large datasets.

\hypertarget{using-read.csv}{%
\subsection{\texorpdfstring{Using \texttt{read.csv()}}{Using read.csv()}}\label{using-read.csv}}

The \textbf{\texttt{read.csv()}} function is straightforward to use. It is actually a special case of the more general \textbf{\texttt{read.table()}} function, with default parameters set for reading CSV files Here's how you can import a CSV file using this function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Importing a CSV file using read.csv()}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"downloads/state\_NV.csv"}\NormalTok{)}

\CommentTok{\# Display the first few rows of the data}
\FunctionTok{head}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

In this example, replace \textbf{\texttt{"downloads/state\_NV.csv"}} with the actual path to your CSV file. The \textbf{\texttt{head()}} function is used to display the first few rows of the imported data.

\hypertarget{details-on-read.csv}{%
\subsubsection*{\texorpdfstring{Details on \texttt{read.csv()}}{Details on read.csv()}}\label{details-on-read.csv}}
\addcontentsline{toc}{subsubsection}{Details on \texttt{read.csv()}}

The \textbf{\texttt{read.csv()}} function is a simplified wrapper around \textbf{\texttt{read.table()}}, with pre-set arguments tailored for reading comma-separated files. Specifically, it sets the following default arguments:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{sep\ =\ ","}} sets the field separator to a comma.
\item
  \textbf{\texttt{header\ =\ TRUE}} indicates that the first line of the file contains column names.
\item
  \textbf{\texttt{stringsAsFactors\ =\ default.stringsAsFactors()}} specifies whether character vectors should be converted to factors (default behavior depends on the R version).
\end{itemize}

Here's an equivalent way to use \texttt{read.table()} to achieve the same result as \texttt{read.csv()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Importing a CSV file using read.table()}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"downloads/state\_NV.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{stringsAsFactors =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Display the first few rows of the data}
\FunctionTok{head}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

As you can see, read.csv() simplifies the process by encapsulating these common settings, making it easier and quicker to read CSV files.

\hypertarget{using-read_csv-from-the-readr-package}{%
\subsection{\texorpdfstring{Using \texttt{read\_csv()} from the \texttt{readr} Package}{Using read\_csv() from the readr Package}}\label{using-read_csv-from-the-readr-package}}

The \textbf{\texttt{readr}} package provides a faster and more convenient way to import CSV files with the \textbf{\texttt{read\_csv()}} function. First, you need to install and load the \textbf{\texttt{readr}} package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install the readr package}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"readr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once the package is installed, you can use the \textbf{\texttt{read\_csv()}} function to import the CSV file:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the readr package}
\FunctionTok{library}\NormalTok{(readr)}

\CommentTok{\# Importing a CSV file using read\_csv()}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"downloads/state\_NV.csv"}\NormalTok{)}

\CommentTok{\# Display the first few rows of the data}
\FunctionTok{head}\NormalTok{(data,}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Similar to \textbf{\texttt{read.csv()}}, replace \textbf{\texttt{"downloads/state\_NV.csv"}} with the actual path to your CSV file. The \textbf{\texttt{read\_csv()}} function also automatically parses the data types of the columns, which can save you time and effort, {you need to be carefull as sometimes read\_csv() may guess the column type wrong!}

\hypertarget{details-on-read_csv}{%
\subsection*{\texorpdfstring{Details on \texttt{read\_csv()}}{Details on read\_csv()}}\label{details-on-read_csv}}
\addcontentsline{toc}{subsection}{Details on \texttt{read\_csv()}}

The \textbf{\texttt{read\_csv()}} function is a special case of the more general \textbf{\texttt{read\_delim()}} function from the \textbf{\texttt{readr}} package, with default parameters set for reading comma-separated files. Specifically, it sets the following default arguments:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{delim\ =\ ","}} sets the field separator to a comma.
\item
  \textbf{\texttt{col\_types\ =\ cols()}} automatically detects the data types of columns unless specified otherwise.
\item
  \textbf{\texttt{trim\_ws\ =\ TRUE}} indicates that whitespace should be trimmed from the beginning and end of each field.
\end{itemize}

These defaults make \textbf{\texttt{read\_csv()}} particularly convenient for reading CSV files without needing to manually specify these common options.

Here's an equivalent way to use \textbf{\texttt{read\_delim()}} to achieve the same result as \textbf{\texttt{read\_csv()}}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Importing a CSV file using read\_delim()}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_delim}\NormalTok{(}
  \StringTok{"downloads/state\_NV.csv"}\NormalTok{,}
  \AttributeTok{delim =} \StringTok{","}\NormalTok{,}
  \AttributeTok{col\_types =} \FunctionTok{cols}\NormalTok{(),}
  \AttributeTok{trim\_ws =} \ConstantTok{TRUE}
\NormalTok{)}

\CommentTok{\# Display the first few rows of the data}
\FunctionTok{head}\NormalTok{(data, }\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As you can see, \textbf{read\_csv()} simplifies the process by encapsulating these common settings, making it easier and quicker to read CSV files.

\hypertarget{handling-parsing-issues}{%
\subsubsection{Handling Parsing Issues}\label{handling-parsing-issues}}

If you been following along, when you ran \textbf{\texttt{data\ \textless{}-\ read\_csv("downloads/state\_NV.csv")}} you have probably encountered a {warning} similar to:

\includegraphics{images/clipboard-3586307356.png}

The warning is letting us know that \textbf{\texttt{read\_csv()}} ran into some parsing issues, and its recommending that we run \textbf{\texttt{problems()}} to see what the issues are. Let's run \textbf{\texttt{problems()}} to see what the issues are:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Display the problems encountered during parsing}
\FunctionTok{problems}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/clipboard-1927136276.png}

The \textbf{\texttt{problems()}} function displays the issues encountered during parsing. The \textbf{`row'} column indicates the row number where the issue occurred, and the \textbf{`col'} column indicates the column number. The \textbf{`expected'} column shows the expected data type, and the \textbf{`actual'} column shows the actual value.

In the example above in row 59, column 44 expected a double but the cell contains \textbf{``\textgreater149''}, which is a character type. A double is a numeric data type that can represent decimal numbers, while a character is a text data type. The issue here is that \textbf{\texttt{read\_csv()}} expected a double but found a character. Even though you can't see it in the table above Column 44 is the \textbf{total\_units} column, which should contain the total number of units for the property. The value \textbf{``\textgreater149''} lets us know that the property has more than 149 units. Therefore the correct column type should be character and not double.

There are two main ways to handle parsing issues in \texttt{read\_csv()}:

\begin{itemize}
\tightlist
\item
  \textbf{Manually Specify Column Types}: You can manually specify the column types using the \texttt{col\_types} argument in \textbf{\texttt{read\_csv()}}. This approach is useful when you know the data types of the columns in advance, but might be cumbersome for large datasets with many columns.
\item
  \textbf{Increase the \texttt{guess\_max} Argument}: You can increase the \texttt{guess\_max} argument in \texttt{read\_csv()} to allow the function to guess the column types for a larger number of rows. This approach isn't perfect, but this way you can avoid having to manually specify the column types.
\end{itemize}

Below is a code example to manually specify the column types:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Manually specify the column types}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}
  \StringTok{"downloads/state\_NV.csv"}\NormalTok{,}
  \AttributeTok{col\_types =} \FunctionTok{cols}\NormalTok{(}
    \AttributeTok{loan\_amount =} \FunctionTok{col\_double}\NormalTok{(),}
    \AttributeTok{total\_units =} \FunctionTok{col\_character}\NormalTok{(),}
    \AttributeTok{.default =} \FunctionTok{col\_character}\NormalTok{(),}
\NormalTok{  ),}
  \AttributeTok{na =} \FunctionTok{c}\NormalTok{(}\StringTok{""}\NormalTok{, }\StringTok{"NA"}\NormalTok{) }\CommentTok{\# This is to specify what is considered a missing value}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

In this code, we manually specify the column types for the \textbf{\texttt{loan\_amount}} and \textbf{\texttt{total\_units}} columns. We also set the default column type to \textbf{\texttt{col\_character()}} to ensure that all other columns are treated as character columns. The \textbf{\texttt{na}} argument specifies the values that should be treated as missing values, which in this case we have set to an empty string and \textbf{``NA''}.

It's also possible to set the \textbf{\texttt{guess\_max}} argument to a higher value to allow \textbf{\texttt{read\_csv()}} to guess the column types for a larger number of rows. This can be useful when you have a large dataset and want to avoid manually specifying the column types. You can even set it to \textbf{\texttt{Inf}} to allow \textbf{\texttt{read\_csv()}} to guess the column types by using all the rows in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Increase the guess\_max argument}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"downloads/state\_NV.csv"}\NormalTok{, }\AttributeTok{guess\_max =} \ConstantTok{Inf}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{handling-file-paths}{%
\subsection{Handling File Paths}\label{handling-file-paths}}

When specifying the path to your CSV file, it's important to ensure that the path is correct. You can use absolute paths or relative paths. Here are some examples:

\begin{itemize}
\tightlist
\item
  \textbf{Absolute Path}: An absolute path specifies the complete path from the root directory. For example, on Windows: \texttt{"C:/Users/YourName/Documents/data.csv"}, or on macOS/Linux: \texttt{"/Users/YourName/Documents/data.csv"}.
\item
  \textbf{Relative Path}: A relative path specifies the path relative to your current working directory. For example, if your current working directory is \texttt{"C:/Users/YourName/Documents"}, you can use \texttt{"data.csv"}.
\end{itemize}

You can check your current working directory in R using the \texttt{getwd()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get the current working directory}
\FunctionTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

You can also set the working directory using the \texttt{setwd()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set the working directory}
\FunctionTok{setwd}\NormalTok{(}\StringTok{"path/to/your/directory"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{common-issues-and-solutions}{%
\subsection{Common Issues and Solutions}\label{common-issues-and-solutions}}

\begin{itemize}
\tightlist
\item
  \textbf{File Not Found Error}: Ensure the file path is correct and the file exists at the specified location.
\item
  \textbf{Incorrect Data Parsing}: If columns are not parsed correctly, you can specify the column types manually using the \texttt{col\_types} argument in \texttt{read\_csv()}.
\item
  \textbf{Missing Values}: R automatically handles missing values as \texttt{NA}. You can customize the handling of missing values using the \texttt{na} argument.
\end{itemize}

By understanding how to import CSV files in R, you can easily load your data and start your data analysis process. In the next sections, we will explore how to clean and manipulate the imported data to prepare it for analysis.

\hypertarget{importing-data-in-chunks}{%
\section{Importing Data in Chunks}\label{importing-data-in-chunks}}

When working with large datasets, it's often necessary to import data in chunks, especially when the dataset is too large to fit into memory or cannot be opened by standard software like Excel. The readr package in R provides a solution with the \textbf{\texttt{read\_delim\_chunked()}} function, which allows for reading a delimited file in manageable chunks.

The \textbf{\texttt{read\_delim\_chunked()}} function operates similarly to \textbf{\texttt{read\_delim()}}, but it processes data in smaller portions, making it easier to handle large datasets. A practical approach is to use a callback function to filter data as each chunk is processed.

Here's an example demonstrating how to import a delimited file in chunks and apply a callback function to filter the data for state\_code ``NV'':

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the readr package}
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(dplyr)}

\CommentTok{\# Define the callback function to filter data for state\_code "NV"}
\NormalTok{filter\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data\_chunk, pos) \{}
  \CommentTok{\# Filters data chunk for only rows where state\_code == "NV"}
\NormalTok{  data\_chunk }\OtherTok{\textless{}{-}}\NormalTok{ data\_chunk}\SpecialCharTok{\%\textgreater{}\%}\FunctionTok{filter}\NormalTok{(state\_code }\SpecialCharTok{==} \StringTok{"NV"}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Import a CSV file in chunks using read\_csv\_chunked()}
\NormalTok{chunked\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_delim\_chunked}\NormalTok{(}
  \StringTok{"downloads/2023\_combined\_mlar\_header.txt"}\NormalTok{, }\CommentTok{\# specify the path to the CSV file}
  \AttributeTok{callback =}\NormalTok{ DataFrameCallback}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(filter\_data), }\CommentTok{\# specify the callback function}
  \AttributeTok{chunk\_size =} \DecValTok{10000}\NormalTok{, }\CommentTok{\# specify the chunk size,}
  \AttributeTok{delim =} \StringTok{"|"}\NormalTok{,}
  \AttributeTok{escape\_double =} \ConstantTok{FALSE}\NormalTok{,}
  \AttributeTok{trim\_ws =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col\_types =} \FunctionTok{cols}\NormalTok{(}\AttributeTok{.default =} \FunctionTok{col\_character}\NormalTok{())}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this example:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{"downloads/2023\_combined\_mlar\_header.txt"}} should be replaced with the actual path to your delimited file.
\item
  \textbf{\texttt{delim\ =\ "\textbar{}"}} specifies the delimiter used in the file.
\item
  \textbf{\texttt{escape\_double\ =\ FALSE}} specifies whether double quotes should be escaped.
\item
  \textbf{\texttt{trim\_ws\ =\ TRUE}} indicates that whitespace should be trimmed from the beginning and end of each field.
\item
  \textbf{\texttt{col\_names\ =\ TRUE}} specifies that the file contains column names.
\item
  \textbf{\texttt{col\_types\ =\ cols(.default\ =\ col\_character())}} sets all columns to be read as character data types.
\end{itemize}

The \textbf{\texttt{filter\_data()}} function is used as a callback to filter the data for the state code ``NV''. A callback function is a function passed as an argument to another function, which is then executed within that function. Here, the \textbf{\texttt{filter\_data()}} function is applied to each chunk of data read by \textbf{\texttt{read\_delim\_chunked()}}, enabling the filtering of data for the state code ``NV'' as it is read in chunks.

\hypertarget{data-exploration-and-cleaning}{%
\chapter{Data Exploration and Cleaning}\label{data-exploration-and-cleaning}}

Once you have data loaded into your R environment, now comes one of the most important parts of the data processing stage, data exploration and cleaning.

\hypertarget{data-exploration}{%
\section*{Data Exploration}\label{data-exploration}}

Data exploration is the initial step in data analysis, where you get a sense of the structure, contents, and characteristics of the dataset. This step involves:

\begin{itemize}
\item
  \textbf{Understanding the Dataset:}
  Reviewing the dataset to understand its structure, the types of data it contains, and the relationships between different variables.
\item
  \textbf{Summary Statistics:}
  Calculating basic statistics such as mean, median, standard deviation, and percentiles to understand the distribution and spread of the data.
\item
  \textbf{Visualization}
  Creating visual representations of the data, such as histograms, box plots, scatter plots, and correlation matrices, to identify patterns, trends, and outliers.
\item
  \textbf{Identifying Data Types}
  Checking the data types of each column to ensure they are as expected (e.g., numerical, categorical, date/time).
\item
  \textbf{Detecting Anomalies}
  Identifying any anomalies, such as missing values, outliers, or inconsistencies that might need to be addressed.
\end{itemize}

\hypertarget{data-cleaning}{%
\section*{Data Cleaning}\label{data-cleaning}}

Data cleaning, also known as data cleansing or scrubbing, involves correcting or removing inaccuracies and inconsistencies in the data to improve its quality. Key steps include:

\begin{itemize}
\item
  \textbf{Handling Missing Values}
  Dealing with missing data by either removing rows/columns with missing values, imputing missing values using statistical methods, or using algorithms that can handle missing data.
\item
  \textbf{Removing Duplicates}
  Identifying and removing duplicate entries to ensure each record is unique.
\item
  \textbf{Correcting Errors}
  Fixing errors such as typos, incorrect data entries, and inconsistent formatting.
\item
  \textbf{Data Transformation}
  Converting data into the appropriate format or structure, such as normalizing or standardizing numerical data, encoding categorical variables, and creating new derived features.
\item
  \textbf{Outlier Treatment}
  Identifying and handling outliers, which may involve removing them or transforming them to reduce their impact.
\item
  \textbf{Consistent Formatting}
  Ensuring consistent formatting across the dataset, such as consistent date formats, uniform case for text data, and standardized units for numerical data.
\end{itemize}

\hypertarget{importance-of-data-exploration-and-cleaning}{%
\section*{Importance of Data Exploration and Cleaning}\label{importance-of-data-exploration-and-cleaning}}

\begin{itemize}
\tightlist
\item
  \textbf{Improves Data Quality:} Ensures the data is accurate, complete, and reliable, which is essential for drawing valid conclusions and making accurate predictions.
\item
  \textbf{Enhances Analysis:} Clean and well-understood data allows for more effective and insightful analysis.
\item
  \textbf{Reduces Errors:} Minimizes the risk of errors and biases in the data, leading to more robust and trustworthy results.
\item
  \textbf{Facilitates Model Building:} Prepares the data in a way that is suitable for building machine learning models, improving their performance and reliability.
\end{itemize}

Overall, data exploration and cleaning are foundational steps that set the stage for successful data analysis and machine learning projects. In this chapter we will go over some of the most common ways to both explore and clean data.

\hypertarget{exploring-and-cleaning-the-data-structure}{%
\section{Exploring and Cleaning the Data Structure}\label{exploring-and-cleaning-the-data-structure}}

In this section we will utilize the HMDA Snapshot data for 2022 in Nevada to practice data structure exploration and cleaning. The data is available at the following link: \url{https://ffiec.cfpb.gov/v2/data-browser-api/view/csv?states=NV\&years=2022}. We have downloaded the data and read it into R using the following:

When we

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the data}
\NormalTok{hmda\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"downloads/state\_NV.csv"}\NormalTok{, }\AttributeTok{guess\_max =} \ConstantTok{Inf}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Where \textbf{\texttt{"downloads/state\_NV.csv"}} would be the path to the downloaded dataset.

\hypertarget{exploring-data-structure}{%
\subsection{Exploring Data Structure}\label{exploring-data-structure}}

One of the first things we should do is to take a look at the structure of the data. This will help us understand the variables and their types. We can do this using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Display the structure of the data}
\FunctionTok{str}\NormalTok{(hmda\_data)}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/structure_printout.png}

The \textbf{str()} function provides a summary of the data frame, including the number of observations and variables, the names of the variables, and the type of each variable. This information is useful for understanding the structure of the data and planning the analysis. In the attached image of the output above, we can see that the data frame has 180204 observations and 99 variables. We can also see that a couple of the columns got assigned incorrect data types by \textbf{\texttt{read\_csv()}}, one of these being \textbf{county\_code} which represents the Federal Information Processing Standards (FIPS) code for the county.

\hypertarget{changing-data-types}{%
\subsection{Changing Data Types}\label{changing-data-types}}

As we saw in the previous section, some of the columns were assigned incorrect data types by \textbf{\texttt{read\_csv()}}. We can fix this by changing the data types of the columns using the \textbf{\texttt{mutate()}} function from the \textbf{dplyr} package. The \textbf{dplyr} package provides a set of functions for data manipulation, and the \textbf{\texttt{mutate()}} function is used to create new columns or modify existing columns.\footnote{You can learn more about the \textbf{\texttt{dplyr}} package at: \url{https://dplyr.tidyverse.org/}} Below we utilize the \textbf{\texttt{mutate()}} function to change the data type of the \textbf{county\_code} column to character:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Change the data types of the columns}
\NormalTok{hmda\_data }\OtherTok{\textless{}{-}}\NormalTok{ hmda\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{county\_code =} \FunctionTok{as.character}\NormalTok{(county\_code))}
\end{Highlighting}
\end{Shaded}

In the code above, we used the \textbf{\texttt{mutate()}} function to change the data type of the \textbf{county\_code} column to character. \textbf{\texttt{as.character()}} is a function that converts the input to a character type, there are other functions like \textbf{\texttt{as.numeric()}} and \textbf{\texttt{as.factor()}} that can be used to convert the input to numeric and factor types respectively.

\hypertarget{using-across-to-change-data-types-for-multiple-columns}{%
\subsection{Using across() to Change Data Types for Multiple Columns}\label{using-across-to-change-data-types-for-multiple-columns}}

When you need to change the data types of multiple columns simultaneously, the \textbf{\texttt{across()}} function in \textbf{\texttt{dplyr}} can be particularly useful. The \textbf{\texttt{across()}} function allows you to apply a function to multiple columns in a \textbf{\texttt{mutate()}} call.

For example, if you want to change the data types of the \textbf{\texttt{census\_tract}}, \textbf{\texttt{action\_taken}}, \textbf{\texttt{loan\_type}}, and loan\_purpose columns to character, you can use the \textbf{\texttt{across()}} function as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Change the data types of multiple columns to character}
\NormalTok{hmda\_data }\OtherTok{\textless{}{-}}\NormalTok{ hmda\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(census\_tract, action\_taken, loan\_type, loan\_purpose), as.character))}
\end{Highlighting}
\end{Shaded}

In this code:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{across(c(census\_tract,\ action\_taken,\ loan\_type,\ loan\_purpose),\ as.character)}} applies the \textbf{\texttt{as.character()}} function to each of the columns listed inside the \textbf{\texttt{across()}} function.
\item
  This approach makes the code more concise and easier to read, especially when dealing with multiple columns.
\end{itemize}

By using \textbf{\texttt{across()}}, you can efficiently change the data types of multiple columns in one step, ensuring that your data is properly formatted for subsequent analysis.

\hypertarget{spotting-and-handling-na-values}{%
\section{Spotting and Handling NA Values}\label{spotting-and-handling-na-values}}

Missing values, represented as \textbf{\texttt{NA}} in R, are a common occurrence in datasets and can significantly impact the results of your data analysis. Therefore, identifying and understanding the extent of missing values is a crucial step in data exploration.

\hypertarget{understanding-na-values}{%
\subsubsection*{Understanding NA Values}\label{understanding-na-values}}

In R, \textbf{\texttt{NA}} (Not Available) is used to represent missing or undefined data. Missing values can arise due to various reasons such as data entry errors, data collection issues, or intentional omissions.

In the HMDA data that we have been working with, all of the reasons above apply. Sometimes financial institutions make errors when submitting the data, they are unable to collect the data for one reason or another, or certain data fields don't apply to certain loan applications.

\hypertarget{why-spotting-na-values-is-important}{%
\subsubsection*{Why Spotting NA Values is Important}\label{why-spotting-na-values-is-important}}

\begin{itemize}
\tightlist
\item
  \textbf{Data Integrity:} Missing values can lead to incorrect conclusions if not handled properly.
\item
  \textbf{Analysis Readiness:} Many statistical and machine learning methods cannot handle missing values directly.
\item
  \textbf{Decision Making:} Identifying the pattern and extent of missing values can inform your strategy for handling them (e.g., imputation, removal).
\end{itemize}

\hypertarget{common-ways-to-spot-na-values}{%
\subsection{Common Ways to Spot NA Values}\label{common-ways-to-spot-na-values}}

Identifying missing values is a critical part of data exploration. Here are some common ways to spot NA values using the \textbf{\texttt{hmda\_data}} dataset that we loaded in.

\hypertarget{checking-for-any-na-values}{%
\subsubsection*{Checking for Any NA Values}\label{checking-for-any-na-values}}
\addcontentsline{toc}{subsubsection}{Checking for Any NA Values}

You can use the \texttt{anyNA()} function to check if there are any NA values in the entire dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check if there are any NA values in the entire dataset}
\NormalTok{any\_na }\OtherTok{\textless{}{-}} \FunctionTok{anyNA}\NormalTok{(hmda\_data)}
\FunctionTok{print}\NormalTok{(any\_na) }\CommentTok{\# Returns TRUE if there are any NA values, otherwise FALSE}
\end{Highlighting}
\end{Shaded}

\hypertarget{counting-na-values-per-column}{%
\subsubsection*{Counting NA Values Per Column}\label{counting-na-values-per-column}}
\addcontentsline{toc}{subsubsection}{Counting NA Values Per Column}

To understand which columns contain NA values and how many, you can use \textbf{\texttt{colSums(is.na())}}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Count the number of NA values in each column }
\NormalTok{na\_per\_column }\OtherTok{\textless{}{-}} \FunctionTok{colSums}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(hmda\_data)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/na_per_column.PNG}

In the script above, the following steps are performed:

\textbf{\texttt{is.na(hmda\_data)}}: This function checks each element of the \textbf{\texttt{hmda\_data}} dataset to see if it is an NA value. It returns a logical matrix of the same dimensions as \textbf{\texttt{hmda\_data}}, where each element is \textbf{\texttt{TRUE}} if the corresponding element in \textbf{\texttt{hmda\_data}} is NA, and \textbf{\texttt{FALSE}} otherwise.

\textbf{\texttt{colSums(is.na(hmda\_data))}}: This function calculates the sum of \textbf{\texttt{TRUE}} values (which are treated as 1) for each column in the logical matrix. As a result, it provides a named vector where each name corresponds to a column in \textbf{\texttt{hmda\_data}}, and each value represents the count of NA values in that column.

\textbf{\texttt{as.data.frame()}}: This function converts the named vector into a data frame. This step is useful for better readability and further manipulation of the results. The resulting data frame has two columns: one for the column names from the original dataset and one for the corresponding counts of NA values.

By running this script, you will obtain a data frame (\textbf{\texttt{na\_per\_column}}) that lists each column in \textbf{\texttt{hmda\_data}} along with the number of NA values it contains. This information is crucial for understanding the extent of missing data in each column, which can guide your decisions on how to handle these missing values in subsequent analysis steps.

\hypertarget{visualizing-na-values-using-visdat-package}{%
\subsection{\texorpdfstring{Visualizing NA Values Using \texttt{visdat} Package}{Visualizing NA Values Using visdat Package}}\label{visualizing-na-values-using-visdat-package}}

Visualizing missing values can provide a quick and intuitive understanding of the extent and distribution of NA values in your dataset. The \textbf{\texttt{visdat}} package in R offers a suite of tools for this purpose. In this section, we'll demonstrate how to visualize NA values using a subsample of the hmda\_data dataset, focusing specifically on the ``property\_value'' and ``loan\_amount'' columns, we do this because the hmda\_data is quite large and \textbf{\texttt{visdat}} can't handle large dataframes well.

\hypertarget{installing-and-loading-the-visdat-package}{%
\subsubsection*{\texorpdfstring{Installing and Loading the \texttt{visdat} Package}{Installing and Loading the visdat Package}}\label{installing-and-loading-the-visdat-package}}
\addcontentsline{toc}{subsubsection}{Installing and Loading the \texttt{visdat} Package}

First, ensure that the \textbf{\texttt{visdat}} package is installed and loaded. If you haven't installed it yet, you can do so with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"visdat"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualizing-na-values}{%
\subsubsection*{Visualizing NA Values}\label{visualizing-na-values}}
\addcontentsline{toc}{subsubsection}{Visualizing NA Values}

To visualize the NA values in the \textbf{``property\_value''} and \textbf{``loan\_amount''} columns of the \textbf{hmda\_data} dataset, you can use the \textbf{\texttt{vis\_dat()}} function. Here's the script to create the visualization:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Loading libraries}
\FunctionTok{library}\NormalTok{(visdat)}
\FunctionTok{library}\NormalTok{(dplyr)}

\CommentTok{\# Visualizing NA values}
\FunctionTok{vis\_dat}\NormalTok{(hmda\_data }\SpecialCharTok{\%\textgreater{}\%}
          \FunctionTok{select}\NormalTok{(loan\_amount, property\_value), }\AttributeTok{warn\_large\_data =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/na_visualization.PNG}

In this script:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{hmda\_data\ \%\textgreater{}\%\ select(loan\_amount,\ property\_value)}}: This line uses the \texttt{select()} function from the \texttt{dplyr} package to create a subsample of the \texttt{hmda\_data} dataset, containing only the ``loan\_amount'' and ``property\_value'' columns.
\item
  \textbf{\texttt{vis\_dat()}}: This function from the \texttt{visdat} package generates a visualization of the dataset, highlighting the NA values.
\item
  \textbf{\texttt{warn\_large\_data\ =\ FALSE}}: This argument suppresses warnings related to large datasets, which is useful when working with large data frames.
\end{itemize}

\hypertarget{understanding-the-visualization}{%
\subsubsection*{Understanding the Visualization}\label{understanding-the-visualization}}

The visualization generated by \textbf{\texttt{vis\_dat()}} provides a color-coded barchart where each bar represents a value in the dataset. The colors indicate different data types or the presence of NA values:

\begin{itemize}
\tightlist
\item
  \textbf{Gray cells}: Represent NA values.
\item
  \textbf{Other colors}: Represent different data types (e.g., numeric, character, etc.).
\end{itemize}

This visual representation makes it easy to spot patterns and concentrations of missing data. For example, you can quickly see if NA values are clustered in certain rows or columns, which might suggest specific reasons for the missing data.

\hypertarget{data-visualization}{%
\chapter{Data Visualization}\label{data-visualization}}

\hypertarget{data-visualization-using-ggplot2}{%
\section{\texorpdfstring{Data Visualization Using \texttt{ggplot2}}{Data Visualization Using ggplot2}}\label{data-visualization-using-ggplot2}}

\hypertarget{introduction-1}{%
\subsection{Introduction}\label{introduction-1}}

Data visualization is a crucial step in data analysis as it helps in understanding the underlying patterns, trends, and relationships in the data. In this chapter, we will explore how to create various types of visualizations using the ggplot2 package in R, focusing on HMDA (Home Mortgage Disclosure Act) data.

\hypertarget{getting-started-with-ggplot2}{%
\subsection{Getting Started with ggplot2}\label{getting-started-with-ggplot2}}

First, ensure you have ggplot2 installed. If not, you can install it using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This will download the latest version of \texttt{ggplot2} from the CRAN repository.

Load the package along with with the HMDA data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)  }\CommentTok{\# For data manipulation}
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{999}\NormalTok{) }\CommentTok{\# To prevent R from printing in scientific notation}

\CommentTok{\# Load HMDA data}
\NormalTok{hmda\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"downloads/state\_NV.csv"}\NormalTok{, }\AttributeTok{guess\_max =} \ConstantTok{Inf}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once we have the data loaded, we proceed to do some preliminary prep and cleanup. The schema for the different data fields avaialable can be found at: \url{https://ffiec.cfpb.gov/documentation/publications/modified-lar/modified-lar-schema}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter and prep HMDA data for plotting}
\NormalTok{filtered\_hmda\_data }\OtherTok{\textless{}{-}}\NormalTok{ hmda\_data}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}
    \CommentTok{\# Filter for only originated transactions}
\NormalTok{    action\_taken }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \CommentTok{\# Filter for only for home purchases}
\NormalTok{    loan\_purpose }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \CommentTok{\# Filter for only primary homss}
\NormalTok{    occupancy\_type }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \CommentTok{\# Filter for primary liens}
\NormalTok{    lien\_status }\SpecialCharTok{==} \DecValTok{1}\NormalTok{,}
    \CommentTok{\# Filter for single unit homes}
\NormalTok{    total\_units }\SpecialCharTok{==} \StringTok{"1"}\NormalTok{,}
    \CommentTok{\# Filter propery value }
    \SpecialCharTok{!}\NormalTok{property\_value }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Exempt"}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \CommentTok{\# Filter income for values below 250 but above 0}
\NormalTok{    income }\SpecialCharTok{\textless{}=}\DecValTok{250} \SpecialCharTok{\&}\NormalTok{ income}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{,}
    \CommentTok{\# Filter for Clark County}
\NormalTok{    county\_code }\SpecialCharTok{==} \StringTok{"32003"}
    
\NormalTok{  )}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{property\_value =} \FunctionTok{as.numeric}\NormalTok{(property\_value),}
    \CommentTok{\# Assigning labels for each loan\_type}
    \AttributeTok{loan\_type =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      loan\_type }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{"Conventional"}\NormalTok{,}
\NormalTok{      loan\_type }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{"FHA"}\NormalTok{,}
\NormalTok{      loan\_type }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{"VA"}\NormalTok{,}
\NormalTok{      loan\_type }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \StringTok{"USDA"}
\NormalTok{    ))}\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# Only keep property values under $1 million}
  \FunctionTok{filter}\NormalTok{(property\_value}\SpecialCharTok{\textless{}}\DecValTok{1000000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{basic-plot-structure}{%
\subsection*{Basic Plot Structure}\label{basic-plot-structure}}

The structure of a ggplot2 plot is built around the \texttt{ggplot()}** function and the \texttt{+} operator to add layers. Here's a simple example of a scatter pplot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ filtered\_hmda\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{y =}\NormalTok{ property\_value))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/basic_plot_structure_img_1.PNG}

In this example, \texttt{ggplot()} is the initial function call to create a new plot. The function takes the following primary arguments:

\begin{itemize}
\tightlist
\item
  \texttt{data}: This argument specifies the dataset to be used in the plot. In this case, \texttt{filtered\_hmda\_data} is the dataset containing the HMDA data.
\item
  \texttt{aes()}: Short for aesthetics, this function defines the mapping of variables in your data to visual properties (aesthetics) such as x and y axes, colors, shapes, and sizes of points or lines. In the example, \texttt{x\ =\ income} maps the \texttt{income} variable to the x-axis, and \texttt{y\ =\ property\_value} maps the \texttt{property\_value} variable to the y-axis.
\end{itemize}

After the initial \texttt{ggplot()} function, we add layers to the plot using the \textbf{\texttt{+}} operator. Each layer represents a specific component of the plot, such as points, lines, bars, etc.

\begin{itemize}
\tightlist
\item
  \texttt{geom\_point()}: This is a geometric object (geom) layer that adds a scatter plot layer to the plot. Each point represents an observation in the dataset.
\end{itemize}

\hypertarget{creating-basic-plot-types-with-ggplot2}{%
\subsection{Creating Basic Plot Types with ggplot2}\label{creating-basic-plot-types-with-ggplot2}}

In this section, we will explore how to create various types of basic plots using ggplot2. We will start with histograms, bar plots, box plots, and line plots, each serving different purposes in data visualization.

\hypertarget{histogram}{%
\subsubsection*{Histogram}\label{histogram}}
\addcontentsline{toc}{subsubsection}{Histogram}

Histograms are useful for visualizing the distribution of a single continuous variable. For example, let's create a histogram to visualize the distribution of property values in our filtered HMDA data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ filtered\_hmda\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ property\_value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{50000}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of Property Values"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Property Value"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/histogram_plot.PNG}

\begin{itemize}
\tightlist
\item
  \texttt{geom\_histogram(binwidth\ =\ 50000,\ fill\ =\ "blue",\ color\ =\ "black")} adds a histogram layer with specified bin width, fill color, and border color.
\item
  \texttt{labs()} is used to add titles and labels to the plot.
\end{itemize}

\hypertarget{bar-plot}{%
\subsubsection*{Bar Plot}\label{bar-plot}}
\addcontentsline{toc}{subsubsection}{Bar Plot}

Bar plots are useful for visualizing categorical data. Let's create a bar plot to visualize the count of loans by loan type.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ filtered\_hmda\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ loan\_type)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Count of Loans by Loan Type"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Loan Type"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{ggplot(data\ =\ filtered\_hmda\_data,\ aes(x\ =\ loan\_type))}: This line initializes the ggplot object with the specified dataset (\texttt{filtered\_hmda\_data}) and maps the loan\_type column to the x-axis.
\item
  \texttt{geom\_bar()}: This adds a bar plot layer to the ggplot object. By default, geom\_bar() counts the number of occurrences of each loan\_type.
\item
  \texttt{labs(title\ =\ "Count\ of\ Loans\ by\ Loan\ Type",\ x\ =\ "Loan\ Type",\ y\ =\ "Count")}: This function is used to add titles and labels to the plot. It specifies the plot title and labels for the x and y axes..
\end{itemize}

\includegraphics{images/bar_plot.PNG}

\hypertarget{box-plot}{%
\subsubsection*{Box Plot}\label{box-plot}}
\addcontentsline{toc}{subsubsection}{Box Plot}

Box plots are useful for visualizing the distribution of a continuous variable across different categories. Let's create a box plot to visualize property values by loan type.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ filtered\_hmda\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ loan\_type, }\AttributeTok{y =}\NormalTok{ property\_value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Property Values by Loan Type"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Loan Type"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Property Value"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/box_plot.PNG}

In this example:

\begin{itemize}
\item
  \texttt{geom\_boxplot()} adds a box plot layer.
\item
  \texttt{labs()} is used to add titles and labels to the plot.
\end{itemize}

\hypertarget{line-plot}{%
\subsubsection*{Line Plot}\label{line-plot}}
\addcontentsline{toc}{subsubsection}{Line Plot}

Line plots are useful for visualizing trends over time or ordered categories. Since the HMDA data we have been working with is not historical, we will use one of R's built-in datasets, AirPassengers, to demonstrate creating a line plot. The \texttt{AirPassengers} dataset contains monthly totals of international airline passengers from 1949 to 1960.

First, let's load the dataset and take a look at its structure:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the AirPassengers dataset}
\FunctionTok{data}\NormalTok{(}\StringTok{"AirPassengers"}\NormalTok{)}
\NormalTok{airpassengers\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Month =} \FunctionTok{time}\NormalTok{(AirPassengers),}
  \AttributeTok{Passengers =} \FunctionTok{as.numeric}\NormalTok{(AirPassengers)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/airplane_table_head.PNG}

The \texttt{airpassengers\_data} dataframe has two columns: \texttt{Month} and \texttt{Passengers}.

\hypertarget{creating-a-line-plot}{%
\subsubsection*{Creating a Line Plot}\label{creating-a-line-plot}}

Now, let's create a line plot to visualize the trend of airline passengers over time.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ airpassengers\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Month, }\AttributeTok{y =}\NormalTok{ Passengers)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Trend of Airline Passengers Over Time"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Month"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of Passengers"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{images/line_graph_example.PNG}

In this example:

\begin{itemize}
\item
  \texttt{ggplot(data\ =\ airpassengers\_data,\ aes(x\ =\ Month,\ y\ =\ Passengers))}: Initializes the ggplot object with the \texttt{airpassengers\_data} dataset and maps the \texttt{Month} column to the x-axis and the \texttt{Passengers} column to the y-axis.
\item
  \texttt{geom\_line(color\ =\ "blue")}: Adds a line plot layer with the line color set to blue.
\item
  \texttt{labs(title\ =\ "Trend\ of\ Airline\ Passengers\ Over\ Time",\ x\ =\ "Month",\ y\ =\ "Number\ of\ Passengers")}: Adds a title and labels to the plot.
\item
  \texttt{theme\_minimal()}: Applies a minimalistic theme to the plot.
\end{itemize}

  \bibliography{book.bib,packages.bib}

\end{document}
